{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96e174c-b915-4e07-bc19-7232534a6452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Databricks / PySpark + Delta Lake\n",
    "# pip install sodapy\n",
    "\n",
    "from sodapy import Socrata\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "import json, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e47e4c-c18a-43e8-995d-43db6c170f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### ─────────────────────────────────────────────\n",
    "### 1. Parámetros y utilidades\n",
    "### ─────────────────────────────────────────────\n",
    "TOKEN          = dbutils.secrets.get(\"claves\", \"token_app\")\n",
    "DATASET_ID     = dbutils.widgets.get(\"codigo_dataset\")                 # p.e. \"4k7j-ux6u\"\n",
    "DELTA_TABLE    = \"main.diplomado.ids_contratos_procesos\"\n",
    "BATCH_SIZE     = 50_000                                                # máximo seguro para SODA 2.0\n",
    "CTRL_PATH      = \"/dbfs/FileStore/tmp/secop_offset.json\"               # para reanudar\n",
    "\n",
    "client = Socrata(\"www.datos.gov.co\", TOKEN, timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d4bdf81-d22d-4251-bd74-39b714c5c0aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer último offset si existe\n",
    "if os.path.exists(CTRL_PATH):\n",
    "    with open(CTRL_PATH) as fh:\n",
    "        offset = json.load(fh).get(\"offset\", 0)\n",
    "else:\n",
    "    offset = 0\n",
    "\n",
    "keys = [\"numero_del_contrato\",\n",
    "        \"numero_de_proceso\",\n",
    "        \"nit_de_la_entidad\",\n",
    "        \"documento_proveedor\",\n",
    "        \"estado_del_proceso\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e400e0-5a3f-4bde-a3c7-baefff81f395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def delta_exists(tbl_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Comprueba si una tabla Delta existe sin tocar la JVM\n",
    "    (funciona en Databricks Serverless).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        _ = DeltaTable.forName(spark, tbl_name)\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd84dc5e-e001-4df1-85ea-d9c736731af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Antes (falla en Serverless)\n",
    "# if not spark._jsparkSession.catalog().tableExists(DELTA_TABLE):\n",
    "\n",
    "# ── Después\n",
    "if not delta_exists(DELTA_TABLE):\n",
    "    (spark\n",
    "       .createDataFrame([], \"\"\"\n",
    "            numero_del_contrato  STRING,\n",
    "            numero_de_proceso    STRING,\n",
    "            nit_de_la_entidad    STRING,\n",
    "            documento_proveedor  STRING,\n",
    "            estado_del_proceso   STRING\n",
    "        \"\"\")\n",
    "       .write.format(\"delta\")\n",
    "       .mode(\"overwrite\")\n",
    "       .saveAsTable(DELTA_TABLE)\n",
    "    )\n",
    "\n",
    "# Ya podemos abrirla con la API Delta estándar\n",
    "delta_tbl = DeltaTable.forName(spark, DELTA_TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f7cbf4-1f73-464b-8d6b-6f963703966a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3️⃣  BUCLE DE PAGINACIÓN, UPSERT Y CONTROL DE OFFSET\n",
    "#     (Serverless-friendly, con retries y back-off)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "#\n",
    "# • Requiere que en el paso 1 ya tengas:\n",
    "#     - client         → Socrata(domain, TOKEN, timeout=180)\n",
    "#     - DATASET_ID     → ID del dataset (widget o string)\n",
    "#     - BATCH_SIZE     → ej. 20_000  (≤ 50 000)\n",
    "#     - keys           → lista de las 5 columnas clave\n",
    "# • Requiere que en el paso 2 ya tengas:\n",
    "#     - delta_tbl      → DeltaTable destino con esas mismas columnas\n",
    "# • Este bloque\n",
    "#     1. crea (si no existe) la tabla de offsets\n",
    "#     2. lee el último offset\n",
    "#     3. descarga lotes con retries y back-off\n",
    "#     4. hace MERGE (upsert) evitando duplicados\n",
    "#     5. actualiza el offset después de cada lote\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "import time\n",
    "from requests.exceptions import ReadTimeout, ConnectionError\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# --------------------- 3.A  tabla de control --------------------\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS main.diplomado.sec_offsets (\n",
    "    dataset_id STRING COMMENT 'Socrata dataset ID',\n",
    "    offset     BIGINT  COMMENT 'Último offset cargado',\n",
    "    updated_at TIMESTAMP\n",
    ") USING DELTA\n",
    "TBLPROPERTIES (delta.minReaderVersion='2', delta.minWriterVersion='5')\n",
    "\"\"\")\n",
    "\n",
    "def get_offset(ds_id: str) -> int:\n",
    "    \"\"\"Devuelve el último offset registrado, o 0 si no existe.\"\"\"\n",
    "    row = (spark.sql(f\"\"\"\n",
    "            SELECT offset\n",
    "            FROM   main.diplomado.sec_offsets\n",
    "            WHERE  dataset_id = '{ds_id}'\n",
    "            ORDER  BY updated_at DESC\n",
    "            LIMIT  1\"\"\")\n",
    "           .collect())\n",
    "    return row[0].offset if row else 0\n",
    "\n",
    "def set_offset(ds_id: str, off: int) -> None:\n",
    "    \"\"\"Inserta o actualiza el offset del dataset.\"\"\"\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO main.diplomado.sec_offsets AS tgt\n",
    "        USING (SELECT '{ds_id}' AS dataset_id,\n",
    "                      {off}     AS offset,\n",
    "                      current_timestamp() AS updated_at) src\n",
    "        ON tgt.dataset_id = src.dataset_id\n",
    "        WHEN MATCHED THEN UPDATE SET offset = src.offset,\n",
    "                                     updated_at = src.updated_at\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "# --------------------- 3.B  parámetros de retries --------------\n",
    "MAX_RETRIES  = 5          # nº de intentos por lote\n",
    "BACKOFF_SECS = 5          # pausa base (aumenta: 5s,10s,15s,…)\n",
    "\n",
    "# --------------------- 3.C  arranque: offset --------------------\n",
    "offset      = get_offset(DATASET_ID)\n",
    "merge_cond  = \" AND \".join([f\"dest.{c}=src.{c}\" for c in keys])\n",
    "\n",
    "# --------------------- 3.D  bucle de paginación -----------------\n",
    "while True:\n",
    "    soql = (f\"select {', '.join(keys)} \"\n",
    "            f\"order by :id \"\n",
    "            f\"limit {BATCH_SIZE} offset {offset}\")\n",
    "\n",
    "    # 3.D-1  Descargar lote con retries\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            batch = client.get(DATASET_ID, query=soql)\n",
    "            break                                      # ✔ éxito\n",
    "        except (ReadTimeout, ConnectionError) as e:\n",
    "            wait = BACKOFF_SECS * attempt              # 5s,10s,…\n",
    "            print(f\"[retry {attempt}/{MAX_RETRIES}] offset {offset:,} – \"\n",
    "                  f\"{type(e).__name__}: {e}; reintentando en {wait}s\")\n",
    "            time.sleep(wait)\n",
    "    else:\n",
    "        raise RuntimeError(f\"❌ agotados {MAX_RETRIES} retries en offset {offset:,}\")\n",
    "\n",
    "    if not batch:          # lote vacío → fin\n",
    "        break\n",
    "\n",
    "    batch_df = spark.createDataFrame(batch)\n",
    "\n",
    "    # 3.D-2  UPSERT (MERGE) para evitar duplicados\n",
    "    (delta_tbl.alias(\"dest\")\n",
    "              .merge(batch_df.alias(\"src\"), merge_cond)\n",
    "              .whenNotMatchedInsertAll()\n",
    "              .execute())\n",
    "\n",
    "    # 3.D-3  Avanza offset y lo persiste\n",
    "    offset += BATCH_SIZE\n",
    "    set_offset(DATASET_ID, offset)\n",
    "\n",
    "print(f\"✅ Descarga completa. Offset final procesado: {offset:,}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "sodapy ==2.2.0"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(20250708) Ingesta Datos API Socrata- Descarga ID",
   "widgets": {
    "codigo_dataset": {
     "currentValue": "rpmr-utcd",
     "nuid": "86d6e821-9d52-47b8-a7e0-28cd629283c9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "rpmr-utcd",
      "label": "",
      "name": "codigo_dataset",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "rpmr-utcd",
      "label": "",
      "name": "codigo_dataset",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
