{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96e174c-b915-4e07-bc19-7232534a6452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sodapy import Socrata\n",
    "\n",
    "#client = Socrata(\"www.datos.gov.co\", None)\n",
    "\n",
    "token = dbutils.secrets.get(\"claves\",\"token_app\")\n",
    "codigo_dataset = dbutils.widgets.get(\"codigo_dataset\")\n",
    "\n",
    "client = Socrata(\"www.datos.gov.co\",token)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e79a0a98-8b26-4f94-863f-3dd183b01d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Parámetros de paginación\n",
    "limit = 500000\n",
    "offset = 0\n",
    "write_mode = \"overwrite\"\n",
    "\n",
    "print(f\"Iniciando carga por lotes para el dataset: {codigo_dataset}\")\n",
    "\n",
    "# 4. Bucle para obtener y cargar los datos por lotes\n",
    "while True:\n",
    "    print(f\"Obteniendo y cargando lote {offset}...\")\n",
    "    # Construye y ejecuta la consulta para el lote actual\n",
    "    query = f\"SELECT numero_del_contrato, numero_de_proceso, nit_de_la_entidad, documento_proveedor, estado_del_proceso LIMIT {limit} OFFSET {offset}\"\n",
    "    results = client.get(codigo_dataset, query=query)\n",
    "\n",
    "    # Si la API no devuelve más registros, se termina el bucle\n",
    "    if not results:\n",
    "        print(\"Carga de datos finalizada.\")\n",
    "        break\n",
    "\n",
    "    # Convierte el lote a un DataFrame de Spark y lo escribe en la tabla Delta\n",
    "    spark.createDataFrame(results).write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(write_mode) \\\n",
    "        .saveAsTable(\"main.diplomado.ids_contratos_procesos_1\")\n",
    "\n",
    "    print(f\"Lote de {len(results)} registros desde offset {offset} cargado.\")\n",
    "\n",
    "    # Se cambia a modo 'append' para las siguientes iteraciones y se incrementa el offset\n",
    "    write_mode = \"append\"\n",
    "    offset += limit\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61071560-f87f-45af-9504-d517fe1da8f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# 3. Parámetros de paginación\n",
    "limit = 500000\n",
    "offset = 0\n",
    "write_mode = \"overwrite\"\n",
    "reintentos=5\n",
    "\n",
    "print(f\"Iniciando carga por lotes para el dataset: {codigo_dataset}\")\n",
    "\n",
    "# 4. Bucle para obtener y cargar los datos por lotes\n",
    "while True:\n",
    "    intentos = 0\n",
    "\n",
    "    while intentos < reintentos:\n",
    "        try:\n",
    "            print(f\"Obteniendo y cargando lote {offset}...\")\n",
    "            # Construye y ejecuta la consulta para el lote actual\n",
    "            query = f\"SELECT numero_del_contrato, numero_de_proceso, nit_de_la_entidad, documento_proveedor, estado_del_proceso LIMIT {limit} OFFSET {offset}\"\n",
    "            results = client.get(codigo_dataset, query=query)\n",
    "\n",
    "            # Si la API no devuelve más registros, se termina el bucle\n",
    "            if not results:\n",
    "                print(\"Carga de datos finalizada.\")\n",
    "                break\n",
    "\n",
    "            # Convierte el lote a un DataFrame de Spark y lo escribe en la tabla Delta\n",
    "            spark.createDataFrame(results).write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(write_mode) \\\n",
    "                .saveAsTable(\"main.diplomado.ids_contratos_procesos_1\")\n",
    "\n",
    "            print(f\"Lote de {len(results)} registros desde offset {offset} cargado.\")\n",
    "\n",
    "            # Se cambia a modo 'append' para las siguientes iteraciones y se incrementa el offset\n",
    "            write_mode = \"append\"\n",
    "            offset += limit\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            intentos +=1\n",
    "            print(\"Error al obtener o cargar el lote:\", e)\n",
    "            print(f\"Intento {intentos} de {reintentos}...\")\n",
    "            time.sleep(10)\n",
    "    else:\n",
    "        print(\"Se alcanzó el número máximo de intentos. Terminando la carga.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94333b20-6d41-4ba0-9157-94de12c421a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# 3. Parámetros de paginación\n",
    "limit = 500000\n",
    "offset = 0\n",
    "write_mode = \"overwrite\"\n",
    "reintentos=5\n",
    "\n",
    "print(f\"Iniciando carga por lotes para el dataset: {codigo_dataset}\")\n",
    "\n",
    "# 4. Bucle para obtener y cargar los datos por lotes\n",
    "while True:\n",
    "    intentos = 0\n",
    "\n",
    "    while intentos < reintentos:\n",
    "        try:\n",
    "            print(f\"Obteniendo y cargando lote {offset}...\")\n",
    "            # Construye y ejecuta la consulta para el lote actual\n",
    "            query = f\"SELECT numero_del_contrato, numero_de_proceso, nit_de_la_entidad, documento_proveedor, estado_del_proceso LIMIT {limit} OFFSET {offset}\"\n",
    "            results = client.get(codigo_dataset, query=query, timeout=240)\n",
    "\n",
    "            # Si la API no devuelve más registros, se termina el bucle\n",
    "            if not results:\n",
    "                print(\"Carga de datos finalizada.\")\n",
    "                break\n",
    "\n",
    "            # Convierte el lote a un DataFrame de Spark y lo escribe en la tabla Delta\n",
    "            spark.createDataFrame(results).write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(write_mode) \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(\"main.diplomado.ids_contratos_procesos_1\")\n",
    "\n",
    "            print(f\"Lote de {len(results)} registros desde offset {offset} cargado.\")\n",
    "\n",
    "            # Se cambia a modo 'append' para las siguientes iteraciones y se incrementa el offset\n",
    "            write_mode = \"append\"\n",
    "            offset += limit\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            intentos +=1\n",
    "            print(\"Error al obtener o cargar el lote:\", e)\n",
    "            print(f\"Intento {intentos} de {reintentos}...\")\n",
    "            time.sleep(10)\n",
    "    else:\n",
    "        print(\"Se alcanzó el número máximo de intentos. Terminando la carga.\")\n",
    "        break\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ed6bf4-a3ef-4115-be80-682ad7ac976f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_secop_id =spark.table(\"main.diplomado.ids_contratos_procesos_1\")\n",
    "df_secop_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41651ee8-df4a-4a63-970d-ed18b2d9b8a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar las funciones necesarias de PySpark\n",
    "from pyspark.sql.functions import sha2, concat_ws, col\n",
    "\n",
    "# 1. Cargar la tabla correcta desde el catálogo a un DataFrame\n",
    "# Se asume que esta es la tabla que contiene las columnas que mencionaste.\n",
    "df_secop_id = spark.table(\"main.diplomado.ids_contratos_procesos_1\")\n",
    "\n",
    "# 2. Definir la lista corregida de columnas para el identificador único\n",
    "columnas_para_hash = [\n",
    "    \"numero_del_contrato\",\n",
    "    \"numero_de_proceso\",\n",
    "    \"nit_de_la_entidad\",\n",
    "    \"documento_proveedor\",\n",
    "    \"estado_del_proceso\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60c6aa8-6ee9-4b07-a59b-8203ceb0ca09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Añadir la nueva columna 'id_unico'\n",
    "# Se concatenan las columnas clave con un separador y se les aplica un hash SHA-2.\n",
    "df_con_id = df_secop_id.withColumn(\n",
    "    \"id_unico_con_estado\",\n",
    "    sha2(concat_ws(\"||\", *[col(c) for c in columnas_para_hash]), 256)\n",
    ")\n",
    "\n",
    "columnas_para_hash_se = [\n",
    "    \"numero_del_contrato\",\n",
    "    \"numero_de_proceso\",\n",
    "    \"nit_de_la_entidad\",\n",
    "    \"documento_proveedor\"\n",
    "]\n",
    "df_con_id = df_con_id.withColumn(\n",
    "    \"id_unico_sin_estado\",\n",
    "    sha2(concat_ws(\"||\", *[col(c) for c in columnas_para_hash_se]), 256)\n",
    ")\n",
    "df_con_id.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ec1ea5-c761-4a47-b8aa-71fb5b7cbe3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_con_id.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"main.diplomado.secop_id_bronze\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "sodapy ==2.2.0"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clase) (20250708) Descarga ID",
   "widgets": {
    "codigo_dataset": {
     "currentValue": "rpmr-utcd",
     "nuid": "86d6e821-9d52-47b8-a7e0-28cd629283c9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "rpmr-utcd",
      "label": "",
      "name": "codigo_dataset",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "rpmr-utcd",
      "label": "",
      "name": "codigo_dataset",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
